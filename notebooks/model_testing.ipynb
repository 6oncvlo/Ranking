{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fa35c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gonpr\\ML_Projects\\Ranking\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "import yaml\n",
    "import optuna\n",
    "\n",
    "from src.data.load import load_data\n",
    "from src.data.prepare import prepare_data\n",
    "from src.models.cv_iterator import leave_last_k\n",
    "from src.data.features import feature_engineering\n",
    "from src.data.utils import build_rank_input\n",
    "from src.models.tuner import BayesianSearch\n",
    "from src.models.evaluator import evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cbdb26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read config\n",
    "with open('../config.yml', 'r') as file:\n",
    "    config=yaml.load(file, Loader= yaml.SafeLoader)\n",
    "del file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de3ceb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare data\n",
    "dfs = load_data(config=config['data_loader'])\n",
    "dfs = prepare_data(dataframes=dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbe7e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "df_train, df_test = leave_last_k(df=dfs['data'], config=config['optimization'])\n",
    "df_train, df_valid = leave_last_k(df=df_train, config=config['optimization'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7335e682",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_item_features = feature_engineering(\n",
    "    dataframes={'user': dfs['user'], 'item': dfs['item'], 'data': df_train}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "442d5f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from src.models.candidate import candidate_generation\n",
    "\n",
    "df_train_neg = candidate_generation(df_train, n=20+3*3+3*3, positive_sampling=False)\n",
    "df_test_neg = df_train_neg.groupby(by=['user_id']).sample(n=3*3)\n",
    "df_train_neg = df_train_neg.drop(df_test_neg.index)\n",
    "df_valid_neg = df_train_neg.groupby(by=['user_id']).sample(n=3*3)\n",
    "df_train_neg = df_train_neg.drop(df_valid_neg.index)\n",
    "\n",
    "df_train = pd.concat([df_train.iloc[:,:3], df_train_neg], ignore_index=True)\n",
    "df_valid = pd.concat([df_valid.iloc[:,:3], df_valid_neg], ignore_index=True)\n",
    "df_test = pd.concat([df_test.iloc[:,:3], df_test_neg], ignore_index=True)\n",
    "del df_train_neg, df_valid_neg, df_test_neg\n",
    "\n",
    "df_train, df_valid = [\n",
    "    build_rank_input(ratings=df, features=user_item_features) for df in (df_train, df_valid)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc2fdc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 15:41:04,354] A new study created in memory with name: no-name-9f8a788a-c0ff-43c0-b3df-a7e6f810d466\n",
      "[I 2025-04-22 15:41:06,591] Trial 0 finished with value: 0.9764391725263232 and parameters: {'learning_rate': 0.051883976570897286, 'gamma': 3.6383090706163106, 'max_depth': 6, 'subsample': 0.7901315430884503, 'n_estimators': 150}. Best is trial 0 with value: 0.9764391725263232.\n",
      "[I 2025-04-22 15:41:09,413] Trial 1 finished with value: 0.9767517045258994 and parameters: {'learning_rate': 0.18470238039771686, 'gamma': 0.10808579587180123, 'max_depth': 7, 'subsample': 0.7585708610689534, 'n_estimators': 172}. Best is trial 1 with value: 0.9767517045258994.\n",
      "[I 2025-04-22 15:41:12,317] Trial 2 finished with value: 0.9768439898977765 and parameters: {'learning_rate': 0.2589001910184663, 'gamma': 4.050699547696411, 'max_depth': 8, 'subsample': 0.8523260747703804, 'n_estimators': 262}. Best is trial 2 with value: 0.9768439898977765.\n",
      "[I 2025-04-22 15:41:14,154] Trial 3 finished with value: 0.9764837159746967 and parameters: {'learning_rate': 0.1428934076147979, 'gamma': 0.20906807206592437, 'max_depth': 6, 'subsample': 0.99660631273111, 'n_estimators': 110}. Best is trial 2 with value: 0.9768439898977765.\n",
      "[I 2025-04-22 15:41:16,521] Trial 4 finished with value: 0.9767121104084858 and parameters: {'learning_rate': 0.16278679711514446, 'gamma': 4.0903757680864965, 'max_depth': 8, 'subsample': 0.8241759352125784, 'n_estimators': 180}. Best is trial 2 with value: 0.9768439898977765.\n",
      "[I 2025-04-22 15:41:17,972] Trial 5 finished with value: 0.9766292066114864 and parameters: {'learning_rate': 0.18087730622202128, 'gamma': 3.9802879415463255, 'max_depth': 5, 'subsample': 0.9228466007265621, 'n_estimators': 93}. Best is trial 2 with value: 0.9768439898977765.\n",
      "[I 2025-04-22 15:41:20,325] Trial 6 finished with value: 0.9761854542468202 and parameters: {'learning_rate': 0.027267584496807842, 'gamma': 4.914227077673981, 'max_depth': 9, 'subsample': 0.9191014247048758, 'n_estimators': 173}. Best is trial 2 with value: 0.9768439898977765.\n",
      "[I 2025-04-22 15:41:23,468] Trial 7 finished with value: 0.9769501312049427 and parameters: {'learning_rate': 0.1761023919439886, 'gamma': 2.322998842146159, 'max_depth': 4, 'subsample': 0.580999601693888, 'n_estimators': 271}. Best is trial 7 with value: 0.9769501312049427.\n",
      "[I 2025-04-22 15:41:25,828] Trial 8 finished with value: 0.9766909044309293 and parameters: {'learning_rate': 0.20703655784996142, 'gamma': 0.016529293745425888, 'max_depth': 3, 'subsample': 0.6284691311674724, 'n_estimators': 154}. Best is trial 7 with value: 0.9769501312049427.\n",
      "[I 2025-04-22 15:41:28,429] Trial 9 finished with value: 0.9762207939249489 and parameters: {'learning_rate': 0.03449377361951084, 'gamma': 2.9196732189104724, 'max_depth': 6, 'subsample': 0.8974446541001642, 'n_estimators': 167}. Best is trial 7 with value: 0.9769501312049427.\n",
      "[I 2025-04-22 15:41:32,115] Trial 10 finished with value: 0.9769449204874517 and parameters: {'learning_rate': 0.10847900838937924, 'gamma': 1.638580945456933, 'max_depth': 3, 'subsample': 0.5012734917642292, 'n_estimators': 295}. Best is trial 7 with value: 0.9769501312049427.\n",
      "[I 2025-04-22 15:41:37,178] Trial 11 finished with value: 0.9772110541500252 and parameters: {'learning_rate': 0.10133704221006602, 'gamma': 1.4777257537910544, 'max_depth': 3, 'subsample': 0.5018699629861345, 'n_estimators': 299}. Best is trial 11 with value: 0.9772110541500252.\n",
      "[I 2025-04-22 15:41:43,185] Trial 12 finished with value: 0.9768301604528927 and parameters: {'learning_rate': 0.09790591265211518, 'gamma': 1.6464090702781862, 'max_depth': 4, 'subsample': 0.5003024680012593, 'n_estimators': 244}. Best is trial 11 with value: 0.9772110541500252.\n",
      "[I 2025-04-22 15:41:49,571] Trial 13 finished with value: 0.9767965412468502 and parameters: {'learning_rate': 0.2486645473830842, 'gamma': 1.756170985040796, 'max_depth': 4, 'subsample': 0.625725063292357, 'n_estimators': 300}. Best is trial 11 with value: 0.9772110541500252.\n",
      "[I 2025-04-22 15:41:55,199] Trial 14 finished with value: 0.9771527268194308 and parameters: {'learning_rate': 0.08136522793858778, 'gamma': 2.4231556866988506, 'max_depth': 4, 'subsample': 0.6124161157427962, 'n_estimators': 224}. Best is trial 11 with value: 0.9772110541500252.\n",
      "[I 2025-04-22 15:42:01,295] Trial 15 finished with value: 0.9770500354402549 and parameters: {'learning_rate': 0.08183921075571679, 'gamma': 0.9488103836977002, 'max_depth': 3, 'subsample': 0.7030587993179508, 'n_estimators': 221}. Best is trial 11 with value: 0.9772110541500252.\n",
      "[I 2025-04-22 15:42:07,605] Trial 16 finished with value: 0.9770543058868991 and parameters: {'learning_rate': 0.12628060604663244, 'gamma': 2.799570906058144, 'max_depth': 5, 'subsample': 0.5822774458411214, 'n_estimators': 220}. Best is trial 11 with value: 0.9772110541500252.\n",
      "[I 2025-04-22 15:42:13,573] Trial 17 finished with value: 0.9767840965130635 and parameters: {'learning_rate': 0.06868168447263763, 'gamma': 1.0559094176030501, 'max_depth': 5, 'subsample': 0.6838847013101228, 'n_estimators': 214}. Best is trial 11 with value: 0.9772110541500252.\n",
      "[I 2025-04-22 15:42:15,594] Trial 18 finished with value: 0.9765880855717483 and parameters: {'learning_rate': 0.12676847107043623, 'gamma': 2.2363078335352764, 'max_depth': 10, 'subsample': 0.5575135346445788, 'n_estimators': 59}. Best is trial 11 with value: 0.9772110541500252.\n",
      "[I 2025-04-22 15:42:20,441] Trial 19 finished with value: 0.9761869104388655 and parameters: {'learning_rate': 0.2879875193893702, 'gamma': 1.0056141330231665, 'max_depth': 4, 'subsample': 0.6649894125732893, 'n_estimators': 249}. Best is trial 11 with value: 0.9772110541500252.\n"
     ]
    }
   ],
   "source": [
    "# perform bayesian search\n",
    "searcher = BayesianSearch(config['optimization']['hyper_params'], algorithm='XGBRanker')\n",
    "\n",
    "def objective(trial) -> float:\n",
    "    return searcher.fit(df_train, df_valid, trial)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9f0a664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      "{'learning_rate': 0.10133704221006602, 'gamma': 1.4777257537910544, 'max_depth': 3, 'subsample': 0.5018699629861345, 'n_estimators': 299}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best trial:\")\n",
    "print(study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9039a41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gonpr\\AppData\\Local\\Temp\\ipykernel_9496\\2778080792.py:4: UserWarning: You are merging on int and float columns where the float values are not equal to their int representation.\n",
      "  df_train = dfs['data'].merge(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# use indicator to know the origin\n",
    "df_train = dfs['data'].merge(\n",
    "    df_test\n",
    "    , on=['user_id', 'item_id', 'rating'], how='left'\n",
    "    , indicator=True\n",
    "    )\n",
    "# keep only rows that are present in df1 but not in df2\n",
    "df_train = df_train[df_train['_merge'] == 'left_only'].drop(columns=['_merge'])\n",
    "\n",
    "user_item_features = feature_engineering(\n",
    "    dataframes={'user': dfs['user'], 'item': dfs['item'], 'data': df_train}\n",
    "    )\n",
    "\n",
    "# add negative sampling\n",
    "df_train_neg = candidate_generation(df_train, n=20+3*3+3*3, positive_sampling=False)\n",
    "df_test_neg = df_train_neg.groupby(by=['user_id']).sample(n=3*3)\n",
    "df_train_neg = df_train_neg.drop(df_test_neg.index)\n",
    "\n",
    "df_train = pd.concat([df_train, df_train_neg], ignore_index=True)\n",
    "df_test = pd.concat([df_test, df_test_neg], ignore_index=True)\n",
    "\n",
    "\n",
    "df_train, df_test = [\n",
    "    build_rank_input(ratings=df.iloc[:,:3], features=user_item_features) for df in (df_train, df_test)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bbc255b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9802685136847599\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ndcg_score\n",
    "from xgboost import XGBRanker\n",
    "import numpy as np\n",
    "\n",
    "clf = XGBRanker(**study.best_trial.params)\n",
    "clf.fit(\n",
    "    df_train['X'], df_train['y'].astype(int)\n",
    "    , group=df_train['group']\n",
    "    , verbose=False\n",
    "    )\n",
    "\n",
    "preds = clf.predict(df_test['X'])\n",
    "\n",
    "print(evaluation(df_test['y'], preds, df_test['group']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24edad59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9168812846621082\n"
     ]
    }
   ],
   "source": [
    "from src.models.baseline import baseline_model\n",
    "\n",
    "result = baseline_model(dataframes=dfs, n=10)\n",
    "group = result.groupby('user_id').size().to_list()\n",
    "\n",
    "print(evaluation(result['rating'], result['est_rating'], group))\n",
    "del group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "032635c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9493807341800834\n"
     ]
    }
   ],
   "source": [
    "df_baseline = build_rank_input(ratings=result.drop(columns=['est_rating']), features=user_item_features)\n",
    "\n",
    "preds = clf.predict(df_baseline['X'])\n",
    "print(evaluation(df_baseline['y'], preds, df_baseline['group']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fa35c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath('..\\..'))\n",
    "\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import optuna\n",
    "\n",
    "from src.data.load import load_data\n",
    "from src.data.prepare import prepare_data\n",
    "from src.models.cv_iterator import leave_last_k\n",
    "from src.features.features import feature_engineering\n",
    "from src.models.retrieval import candidate_generation\n",
    "from src.features.utils import build_rank_input\n",
    "from src.models.tuner import BayesianSearch\n",
    "from src.models.ranker import Ranker\n",
    "from src.models.evaluator import evaluation, recs_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cbdb26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read config\n",
    "with open('..\\config.yml', 'r') as file:\n",
    "    config=yaml.load(file, Loader= yaml.SafeLoader)\n",
    "del file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de3ceb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and prepare data\n",
    "dfs = load_data(config=config['data_loader'])\n",
    "dfs = prepare_data(dataframes=dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbe7e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train-test split\n",
    "df_train, df_test = leave_last_k(df=dfs['data'], config=config['optimization'])\n",
    "df_train, df_valid = leave_last_k(df=df_train, config=config['optimization'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eb620d",
   "metadata": {},
   "source": [
    "**Model Building**\n",
    "- Candidate Generation: in addition to the existing observations, it retrieves items that a user might like/dislike it (negative sampling)\n",
    "- Feature Engineering: creates cross user-item features for ranking model\n",
    "- Hyper-parameter Tunning: searches best hyper-parameters to maximize evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "442d5f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate candidates through negative sampling\n",
    "candidates = candidate_generation(df_train, config['optimization']['retrieval'])\n",
    "df_train = pd.concat([df_train.iloc[:,:3], candidates['positive'], candidates['negative']], ignore_index=True)\n",
    "\n",
    "del candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d85b542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build features for ranking model\n",
    "user_item_features = feature_engineering(\n",
    "    dataframes={'user': dfs['user'], 'item': dfs['item'], 'data': df_train}\n",
    "    )\n",
    "\n",
    "df_train, df_valid = [\n",
    "    build_rank_input(ratings=df.iloc[:,:3], features=user_item_features)\n",
    "    for df in (df_train, df_valid)\n",
    "    ]\n",
    "\n",
    "del user_item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc2fdc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-12 21:43:31,941] A new study created in memory with name: no-name-46fb3476-52ec-4a58-abd9-651f48347d1c\n",
      "[I 2025-05-12 21:43:46,903] Trial 0 finished with value: 0.9464911166587041 and parameters: {'learning_rate': 0.2836990601615435, 'gamma': 8.90023881537862, 'max_depth': 13, 'subsample': 0.584260145764142, 'n_estimators': 294}. Best is trial 0 with value: 0.9464911166587041.\n",
      "[I 2025-05-12 21:43:58,585] Trial 1 finished with value: 0.9468276038883875 and parameters: {'learning_rate': 0.08314398871405618, 'gamma': 6.558256906498968, 'max_depth': 7, 'subsample': 0.6903977001584306, 'n_estimators': 228}. Best is trial 1 with value: 0.9468276038883875.\n",
      "[I 2025-05-12 21:44:13,813] Trial 2 finished with value: 0.9459845749388813 and parameters: {'learning_rate': 0.4972996872071615, 'gamma': 8.888728848030302, 'max_depth': 14, 'subsample': 0.7614398051061037, 'n_estimators': 298}. Best is trial 1 with value: 0.9468276038883875.\n",
      "[I 2025-05-12 21:44:25,217] Trial 3 finished with value: 0.9462901662674825 and parameters: {'learning_rate': 0.4602592076832617, 'gamma': 5.722616195685971, 'max_depth': 15, 'subsample': 0.9234543335021622, 'n_estimators': 245}. Best is trial 1 with value: 0.9468276038883875.\n",
      "[I 2025-05-12 21:44:39,611] Trial 4 finished with value: 0.944941703888366 and parameters: {'learning_rate': 0.44606866466126716, 'gamma': 8.412302070821315, 'max_depth': 7, 'subsample': 0.9315474717565833, 'n_estimators': 303}. Best is trial 1 with value: 0.9468276038883875.\n",
      "[I 2025-05-12 21:44:49,315] Trial 5 finished with value: 0.9465050136963244 and parameters: {'learning_rate': 0.26049086174053787, 'gamma': 9.371443698899975, 'max_depth': 15, 'subsample': 0.5367315551108129, 'n_estimators': 200}. Best is trial 1 with value: 0.9468276038883875.\n",
      "[I 2025-05-12 21:45:01,067] Trial 6 finished with value: 0.9462046522586147 and parameters: {'learning_rate': 0.03897497278281166, 'gamma': 0.7336708991823038, 'max_depth': 7, 'subsample': 0.7948989605597752, 'n_estimators': 130}. Best is trial 1 with value: 0.9468276038883875.\n",
      "[I 2025-05-12 21:45:06,647] Trial 7 finished with value: 0.945741277731157 and parameters: {'learning_rate': 0.47800848924275896, 'gamma': 9.90700744991557, 'max_depth': 11, 'subsample': 0.5549836965669526, 'n_estimators': 79}. Best is trial 1 with value: 0.9468276038883875.\n",
      "[I 2025-05-12 21:45:24,553] Trial 8 finished with value: 0.9463657424510591 and parameters: {'learning_rate': 0.29535055541402017, 'gamma': 6.005448258286373, 'max_depth': 14, 'subsample': 0.959324138035444, 'n_estimators': 377}. Best is trial 1 with value: 0.9468276038883875.\n",
      "[I 2025-05-12 21:45:35,531] Trial 9 finished with value: 0.9474643969766523 and parameters: {'learning_rate': 0.061194601714346195, 'gamma': 4.360445092557902, 'max_depth': 7, 'subsample': 0.5765525819702073, 'n_estimators': 181}. Best is trial 9 with value: 0.9474643969766523.\n",
      "[I 2025-05-12 21:45:44,118] Trial 10 finished with value: 0.9475550093214796 and parameters: {'learning_rate': 0.14045655991849348, 'gamma': 2.714571817918701, 'max_depth': 3, 'subsample': 0.6583254310218509, 'n_estimators': 148}. Best is trial 10 with value: 0.9475550093214796.\n",
      "[I 2025-05-12 21:45:52,953] Trial 11 finished with value: 0.9482365559791822 and parameters: {'learning_rate': 0.1313766817581975, 'gamma': 2.527344368283234, 'max_depth': 3, 'subsample': 0.6530051385723286, 'n_estimators': 142}. Best is trial 11 with value: 0.9482365559791822.\n",
      "[I 2025-05-12 21:45:58,781] Trial 12 finished with value: 0.9476915178612026 and parameters: {'learning_rate': 0.16808477197377047, 'gamma': 2.277029858081036, 'max_depth': 3, 'subsample': 0.6750730062682091, 'n_estimators': 59}. Best is trial 11 with value: 0.9482365559791822.\n",
      "[I 2025-05-12 21:46:05,043] Trial 13 finished with value: 0.947209067995996 and parameters: {'learning_rate': 0.16618064984208974, 'gamma': 1.3753758907345104, 'max_depth': 3, 'subsample': 0.6577424861660383, 'n_estimators': 53}. Best is trial 11 with value: 0.9482365559791822.\n",
      "[I 2025-05-12 21:46:12,423] Trial 14 finished with value: 0.947612307250431 and parameters: {'learning_rate': 0.18464081600985938, 'gamma': 2.9912992912675036, 'max_depth': 5, 'subsample': 0.8258380962293468, 'n_estimators': 103}. Best is trial 11 with value: 0.9482365559791822.\n",
      "[I 2025-05-12 21:46:21,124] Trial 15 finished with value: 0.9474747839784743 and parameters: {'learning_rate': 0.20349998000245617, 'gamma': 2.3493890508249033, 'max_depth': 5, 'subsample': 0.717895672335056, 'n_estimators': 126}. Best is trial 11 with value: 0.9482365559791822.\n",
      "[I 2025-05-12 21:46:26,225] Trial 16 finished with value: 0.9459045001462343 and parameters: {'learning_rate': 0.34599802615363306, 'gamma': 3.9821157857557923, 'max_depth': 5, 'subsample': 0.6163624307984299, 'n_estimators': 54}. Best is trial 11 with value: 0.9482365559791822.\n",
      "[I 2025-05-12 21:46:47,333] Trial 17 finished with value: 0.9443725712161226 and parameters: {'learning_rate': 0.10725790257353149, 'gamma': 0.07744076665507693, 'max_depth': 10, 'subsample': 0.8473242732743309, 'n_estimators': 166}. Best is trial 11 with value: 0.9482365559791822.\n",
      "[I 2025-05-12 21:46:53,913] Trial 18 finished with value: 0.9460208647051489 and parameters: {'learning_rate': 0.020318217123326293, 'gamma': 1.4682569738903997, 'max_depth': 3, 'subsample': 0.7352312574053623, 'n_estimators': 96}. Best is trial 11 with value: 0.9482365559791822.\n",
      "[I 2025-05-12 21:47:00,955] Trial 19 finished with value: 0.9462061373090476 and parameters: {'learning_rate': 0.3510637100619334, 'gamma': 3.8906191083415473, 'max_depth': 9, 'subsample': 0.5150584150922884, 'n_estimators': 106}. Best is trial 11 with value: 0.9482365559791822.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial:\n",
      " {'learning_rate': 0.1313766817581975, 'gamma': 2.527344368283234, 'max_depth': 3, 'subsample': 0.6530051385723286, 'n_estimators': 142}\n"
     ]
    }
   ],
   "source": [
    "# hyper-parameter tunning through bayesian search\n",
    "searcher = BayesianSearch(config['optimization']['hyper_params'], algorithm='XGBRanker')\n",
    "\n",
    "def objective(trial) -> float:\n",
    "    return searcher.fit(df_train, df_valid, trial)\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best trial:\\n\", study.best_trial.params)\n",
    "del searcher"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96271074",
   "metadata": {},
   "source": [
    "**Evaluation**\n",
    "- Use best hyper-params to train on previous train and validation sets\n",
    "- Check model accuracy on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9039a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get anti test-set, i.e., train & validation sets together\n",
    "df_train = dfs['data'].merge(\n",
    "    df_test\n",
    "    , on=['user_id', 'item_id', 'rating'], how='left'\n",
    "    , indicator=True\n",
    "    )\n",
    "df_train = df_train[df_train['_merge'] == 'left_only'].drop(columns=['_merge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56ac30fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate candidates\n",
    "candidates = candidate_generation(df_train, config['optimization']['retrieval'])\n",
    "df_train_ = pd.concat([df_train.iloc[:,:3], candidates['positive'], candidates['negative']], ignore_index=True)\n",
    "\n",
    "del candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "39b622ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create features for ranking model\n",
    "user_item_features = feature_engineering(\n",
    "    dataframes={'user': dfs['user'], 'item': dfs['item'], 'data': df_train_}\n",
    "    )\n",
    "\n",
    "df_train_, df_test_ = [\n",
    "    build_rank_input(ratings=df.iloc[:,:3], features=user_item_features) for df in (df_train_, df_test)\n",
    "    ]\n",
    "\n",
    "del user_item_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbc255b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NDCG: 0.9527302471389516\n"
     ]
    }
   ],
   "source": [
    "clf = Ranker(algorithm='XGBRanker', params=study.best_trial.params)\n",
    "clf.fit(\n",
    "    df_train_['X'], df_train_['y'].astype(int)\n",
    "    , group=df_train_['group']\n",
    "    )\n",
    "\n",
    "preds = clf.predict(df_test_['X'])\n",
    "\n",
    "print(f\"NDCG: {evaluation(df_test_['y'], preds, df_test_['group'])}\")\n",
    "\n",
    "# shouldn't be done with test set\n",
    "# recs_score(df_test.iloc[:, :2], df_train.iloc[:, :3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
